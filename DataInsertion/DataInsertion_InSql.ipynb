{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!curl ifconfig.me"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlsqi6i_AEtQ",
        "outputId": "d93af753-20f0-4743-deef-fe306e6d87d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.199.55.132"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wobeFG-_vrF",
        "outputId": "84c1386e-9a59-4631-ef3b-7cc28d05a956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:4 https://packages.microsoft.com/ubuntu/22.04/prod jammy InRelease [3,632 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,461 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,572 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,139 kB]\n",
            "Get:14 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main armhf Packages [20.3 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:16 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main amd64 Packages [232 kB]\n",
            "Get:17 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main all Packages [1,252 B]\n",
            "Get:18 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main arm64 Packages [69.7 kB]\n",
            "Get:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,840 kB]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,128 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,148 kB]\n",
            "Get:22 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,764 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,267 kB]\n",
            "Get:24 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,932 kB]\n",
            "Fetched 34.0 MB in 3s (11.3 MB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  odbcinst unixodbc\n",
            "The following NEW packages will be installed:\n",
            "  msodbcsql18 odbcinst unixodbc\n",
            "0 upgraded, 3 newly installed, 0 to remove and 36 not upgraded.\n",
            "Need to get 791 kB of archives.\n",
            "After this operation, 164 kB of additional disk space will be used.\n",
            "Get:1 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main amd64 msodbcsql18 amd64 18.5.1.1-1 [755 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 odbcinst amd64 2.3.9-5ubuntu0.1 [9,930 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 unixodbc amd64 2.3.9-5ubuntu0.1 [26.7 kB]\n",
            "Fetched 791 kB in 0s (1,905 kB/s)\n",
            "Selecting previously unselected package odbcinst.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 126281 files and directories currently installed.)\r\n",
            "Preparing to unpack .../odbcinst_2.3.9-5ubuntu0.1_amd64.deb ...\r\n",
            "Unpacking odbcinst (2.3.9-5ubuntu0.1) ...\r\n",
            "Selecting previously unselected package unixodbc.\r\n",
            "Preparing to unpack .../unixodbc_2.3.9-5ubuntu0.1_amd64.deb ...\r\n",
            "Unpacking unixodbc (2.3.9-5ubuntu0.1) ...\r\n",
            "Selecting previously unselected package msodbcsql18.\r\n",
            "Preparing to unpack .../msodbcsql18_18.5.1.1-1_amd64.deb ...\r\n",
            "debconf: unable to initialize frontend: Dialog\r\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\r\n",
            "debconf: falling back to frontend: Readline\r\n",
            "Unpacking msodbcsql18 (18.5.1.1-1) ...\r\n",
            "Setting up odbcinst (2.3.9-5ubuntu0.1) ...\r\n",
            "Setting up unixodbc (2.3.9-5ubuntu0.1) ...\r\n",
            "Setting up msodbcsql18 (18.5.1.1-1) ...\r\n",
            "odbcinst: Driver installed. Usage count increased to 1. \r\n",
            "    Target directory is /etc\r\n",
            "Processing triggers for man-db (2.10.2-1) ...\r\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   975  100   975    0     0   6872      0 --:--:-- --:--:-- --:--:--  6914\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100    88  100    88    0     0    681      0 --:--:-- --:--:-- --:--:--   687\n",
            "W: https://packages.microsoft.com/ubuntu/22.04/prod/dists/jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n"
          ]
        }
      ],
      "source": [
        "%%sh\n",
        "curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\n",
        "curl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list > /etc/apt/sources.list.d/mssql-release.list\n",
        "sudo apt-get update\n",
        "sudo ACCEPT_EULA=Y apt-get -q -y install msodbcsql18"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install unixodbc-dev\n",
        "!pip install pyodbc\n",
        "!odbcinst -q -d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjybAmti_y0p",
        "outputId": "98952113-0aa6-45eb-8529-5dd8ff8e3838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "unixodbc-dev is already the newest version (2.3.9-5ubuntu0.1).\n",
            "unixodbc-dev set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n",
            "Collecting pyodbc\n",
            "  Downloading pyodbc-5.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Downloading pyodbc-5.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (346 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.2/346.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyodbc\n",
            "Successfully installed pyodbc-5.2.0\n",
            "[ODBC Driver 18 for SQL Server]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Aofjgte_0_w",
        "outputId": "550ba918-7c5e-4666-c82d-67957c09376a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyodbc\n",
        "import time"
      ],
      "metadata": {
        "id": "XSIFJOgJ_1Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_insert(cursor, query, data, batch_size=50000):\n",
        "    cursor.fast_executemany = True\n",
        "    for start in range(0, len(data), batch_size):\n",
        "        end = start + batch_size\n",
        "        cursor.executemany(query, data[start:end])\n",
        "\n",
        "start = time.time()\n",
        "print(\"[DEBUG] Starting ETL process...\")\n",
        "\n",
        "# Step 1: Load and combine both CSVs\n",
        "print(\"[DEBUG] Loading and combining CSVs...\")\n",
        "df1 = pd.read_csv(\"/content/drive/MyDrive/cleaned_data_stock_market/nasdaq/cleaned_combined_data.csv\")\n",
        "df2 = pd.read_csv(\"/content/drive/MyDrive/cleaned_data_stock_market/nyse/cleaned_combined_data_nyse.csv\")\n",
        "df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# # Trim the data to 10,000 rows for testing/performance\n",
        "# if len(df) > 5000000:\n",
        "#     df = df.iloc[:5000000].copy()\n",
        "\n",
        "# Step 2: Clean column names\n",
        "print(\"[DEBUG] Cleaning column names...\")\n",
        "df.columns = [col.lower().strip().replace(\" \", \"_\") for col in df.columns]\n",
        "\n",
        "# Step 3: Parse date column and drop NaT rows\n",
        "print(\"[DEBUG] Parsing date column...\")\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"], dayfirst=True, errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"date\"])\n",
        "\n",
        "# Step 4: Create DimDate\n",
        "print(\"[DEBUG] Creating DimDate...\")\n",
        "dim_date = df[\"date\"].drop_duplicates().to_frame()\n",
        "dim_date[\"date_id\"] = dim_date[\"date\"].dt.strftime('%Y%m%d').astype(int)\n",
        "dim_date[\"day\"] = dim_date[\"date\"].dt.day\n",
        "dim_date[\"month\"] = dim_date[\"date\"].dt.month\n",
        "dim_date[\"month_name\"] = dim_date[\"date\"].dt.strftime('%B')\n",
        "dim_date[\"quarter\"] = dim_date[\"date\"].dt.quarter\n",
        "dim_date[\"year\"] = dim_date[\"date\"].dt.year\n",
        "dim_date[\"weekday\"] = dim_date[\"date\"].dt.day_name()\n",
        "# Keep 'date' column for merging\n",
        "# dim_date = dim_date[[\"date_id\", \"day\", \"month\", \"month_name\", \"quarter\", \"year\", \"weekday\"]]\n",
        "\n",
        "# Debug: Check for duplicate date_id values before inserting into DimDate\n",
        "num_duplicates = dim_date['date_id'].duplicated().sum()\n",
        "print(f\"[DEBUG] Number of duplicate date_id values in dim_date: {num_duplicates}\")\n",
        "print(\"[DEBUG] First 5 rows of dim_date:\")\n",
        "print(dim_date.head())\n",
        "\n",
        "# Step 5: Create DimCompany\n",
        "dim_company = df[\"company_symbol\"].drop_duplicates().to_frame()\n",
        "dim_company[\"company_id\"] = range(1, len(dim_company) + 1)\n",
        "dim_company = dim_company.rename(columns={\"company_symbol\": \"ticker\"})\n",
        "dim_company = dim_company[[\"company_id\", \"ticker\"]]\n",
        "\n",
        "# Step 6: Merge surrogate keys\n",
        "df = df.merge(dim_date, left_on=\"date\", right_on=\"date\", how=\"left\")\n",
        "df = df.merge(dim_company, left_on=\"company_symbol\", right_on=\"ticker\", how=\"left\")\n",
        "\n",
        "# After merging, drop 'date' column from dim_date before insert\n",
        "insert_dim_date = dim_date[[\"date_id\", \"day\", \"month\", \"month_name\", \"quarter\", \"year\", \"weekday\"]]\n",
        "\n",
        "# Step 7: Create FactStockPrice data\n",
        "fact_df = df[[\n",
        "    \"date_id\", \"company_id\", \"open\", \"high\", \"low\",\n",
        "    \"close\", \"adjusted_close\", \"volume\"\n",
        "]].copy()\n",
        "\n",
        "# Rename for consistency with DB column names\n",
        "fact_df.rename(columns={\n",
        "    \"open\": \"open_price\",\n",
        "    \"high\": \"high_price\",\n",
        "    \"low\": \"low_price\",\n",
        "    \"close\": \"close_price\"\n",
        "}, inplace=True)\n",
        "\n",
        "# Connect to Azure SQL and insert test rows\n",
        "print(\"[DEBUG] Connecting to Azure SQL...\")\n",
        "conn = pyodbc.connect(\n",
        "    'DRIVER={ODBC Driver 18 for SQL Server};'\n",
        "    'SERVER=dbadt.database.windows.net;'\n",
        "    'DATABASE=adt-stock-datawarehouse;UID=vradmin;PWD={Password};'\n",
        "    'Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;'\n",
        ")\n",
        "print(\"[DEBUG] Connected to Azure SQL.\")\n",
        "\n",
        "with conn.cursor() as cur:\n",
        "    print(\"[DEBUG] Inserting into DimDate...\")\n",
        "    batch_insert(\n",
        "        cur,\n",
        "        \"INSERT INTO DimDate (date_id, day, month, month_name, quarter, year, weekday) VALUES (?, ?, ?, ?, ?, ?, ?)\",\n",
        "        insert_dim_date.values.tolist()\n",
        "    )\n",
        "    conn.commit()\n",
        "    print(\"[DEBUG] DimDate insert complete.\")\n",
        "\n",
        "    print(\"[DEBUG] Inserting into DimCompany...\")\n",
        "    batch_insert(\n",
        "        cur,\n",
        "        \"INSERT INTO DimCompany (company_id, ticker) VALUES (?, ?)\",\n",
        "        dim_company.values.tolist()\n",
        "    )\n",
        "    conn.commit()\n",
        "    print(\"[DEBUG] DimCompany insert complete.\")\n",
        "\n",
        "    print(\"[DEBUG] Inserting into FactStockPrice...\")\n",
        "    batch_insert(\n",
        "        cur,\n",
        "        \"\"\"INSERT INTO FactStockPrice (\n",
        "            date_id, company_id, open_price, high_price, low_price,\n",
        "            close_price, adjusted_close, volume\n",
        "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n",
        "        fact_df.values.tolist()\n",
        "    )\n",
        "    conn.commit()\n",
        "    print(\"[DEBUG] FactStockPrice insert complete.\")\n",
        "\n",
        "conn.close()\n",
        "end = time.time()\n",
        "print(f\"✅ All tables populated successfully in {end - start:.2f} seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LThZicEX_1MK",
        "outputId": "337b27f9-cb44-4cc0-cf57-65d8e60a10d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Starting ETL process...\n",
            "[DEBUG] Loading and combining CSVs...\n",
            "[DEBUG] Cleaning column names...\n",
            "[DEBUG] Parsing date column...\n",
            "[DEBUG] Creating DimDate...\n",
            "[DEBUG] Number of duplicate date_id values in dim_date: 0\n",
            "[DEBUG] First 5 rows of dim_date:\n",
            "        date   date_id  day  month month_name  quarter  year    weekday\n",
            "0 2005-09-27  20050927   27      9  September        3  2005    Tuesday\n",
            "1 2005-09-28  20050928   28      9  September        3  2005  Wednesday\n",
            "2 2005-09-29  20050929   29      9  September        3  2005   Thursday\n",
            "3 2005-09-30  20050930   30      9  September        3  2005     Friday\n",
            "4 2005-10-03  20051003    3     10    October        4  2005     Monday\n",
            "[DEBUG] Connecting to Azure SQL...\n",
            "[DEBUG] Connected to Azure SQL.\n",
            "[DEBUG] Inserting into DimDate...\n",
            "[DEBUG] DimDate insert complete.\n",
            "[DEBUG] Inserting into DimCompany...\n",
            "[DEBUG] DimCompany insert complete.\n",
            "[DEBUG] Inserting into FactStockPrice...\n",
            "[DEBUG] FactStockPrice insert complete.\n",
            "✅ All tables populated successfully in 694.77 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyodbc\n",
        "import time\n",
        "import traceback\n",
        "\n",
        "def batch_insert(cursor, query, data, batch_size=100):\n",
        "    cursor.fast_executemany = True\n",
        "    for start in range(0, len(data), batch_size):\n",
        "        end = min(start + batch_size, len(data))\n",
        "        batch = data[start:end]\n",
        "        try:\n",
        "            cursor.executemany(query, batch)\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Batch insert failed for rows {start}-{end}: {e}\")\n",
        "            traceback.print_exc()\n",
        "            # Try row-by-row insert for debug\n",
        "            for i, row in enumerate(batch, start=start):\n",
        "                try:\n",
        "                    cursor.execute(query, row)\n",
        "                except Exception as row_err:\n",
        "                    print(f\"    [ROW ERROR] Row {i}: {row_err} | Data: {row}\")\n",
        "                    traceback.print_exc()\n",
        "\n",
        "start = time.time()\n",
        "print(\"[DEBUG] Starting full News ETL with table creation...\")\n",
        "\n",
        "# Load news data\n",
        "news_df = pd.read_csv(\"cleaned_final_df.csv\")\n",
        "news_df.columns = [\"source_url\", \"article\", \"ticker_symbol\", \"company_name\", \"published_at\", \"sentiment\"]\n",
        "news_df['published_at'] = pd.to_datetime(news_df['published_at'], errors='coerce')\n",
        "news_df = news_df.dropna(subset=['published_at'])\n",
        "\n",
        "# Truncate long text columns (source_url only since article will be dropped)\n",
        "news_df['source_url'] = news_df['source_url'].astype(str).str.slice(0, 2048)\n",
        "\n",
        "news_df['date_id'] = news_df['published_at'].dt.strftime('%Y%m%d').astype(int)\n",
        "\n",
        "# Connect to Azure SQL\n",
        "conn = pyodbc.connect(\n",
        "    'DRIVER={ODBC Driver 18 for SQL Server};'\n",
        "    'SERVER=dbadt.database.windows.net;'\n",
        "    'DATABASE=adt-stock-datawarehouse;UID=vradmin;PWD={Password};'\n",
        "    'Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;'\n",
        ")\n",
        "\n",
        "# 1. Create DimSentiment if it does not exist\n",
        "with conn.cursor() as cur:\n",
        "    cur.execute(\"\"\"\n",
        "        IF NOT EXISTS (\n",
        "            SELECT * FROM INFORMATION_SCHEMA.TABLES\n",
        "            WHERE TABLE_NAME = 'DimSentiment'\n",
        "        )\n",
        "        BEGIN\n",
        "            CREATE TABLE DimSentiment (\n",
        "                sentiment_id INT PRIMARY KEY,\n",
        "                sentiment VARCHAR(20) UNIQUE NOT NULL\n",
        "            )\n",
        "        END\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "\n",
        "# 2. Insert unique sentiment labels\n",
        "sentiments = pd.DataFrame({'sentiment': ['positive', 'negative', 'neutral']})\n",
        "with conn.cursor() as cur:\n",
        "    cur.execute(\"SELECT sentiment FROM DimSentiment\")\n",
        "    existing_sentiments = set(row[0].lower() for row in cur.fetchall())\n",
        "    new_sentiments = sentiments[~sentiments['sentiment'].isin(existing_sentiments)]\n",
        "    if not new_sentiments.empty:\n",
        "        new_sentiments['sentiment_id'] = range(1, len(new_sentiments) + 1)\n",
        "        batch_insert(\n",
        "            cur,\n",
        "            \"INSERT INTO DimSentiment (sentiment_id, sentiment) VALUES (?, ?)\",\n",
        "            new_sentiments[['sentiment_id', 'sentiment']].values.tolist()\n",
        "        )\n",
        "        conn.commit()\n",
        "\n",
        "# 3. Update DimCompany to include company_name\n",
        "news_companies = news_df[['ticker_symbol', 'company_name']].drop_duplicates()\n",
        "\n",
        "with conn.cursor() as cur:\n",
        "    cur.execute(\"SELECT company_id, ticker FROM DimCompany\")\n",
        "    columns = [desc[0] for desc in cur.description]\n",
        "    rows = cur.fetchall()\n",
        "    print(f\"[DEBUG] Column names in DimCompany: {columns}\")\n",
        "    print(f\"[DEBUG] Sample row: {rows[0] if rows else 'No rows found'}\")\n",
        "    existing = pd.DataFrame.from_records(rows, columns=columns)\n",
        "\n",
        "merged = pd.merge(existing, news_companies, how='left', left_on='ticker', right_on='ticker_symbol')\n",
        "merged = merged.dropna(subset=[\"company_name\"]).drop_duplicates()\n",
        "\n",
        "with conn.cursor() as cur:\n",
        "    for row in merged.itertuples(index=False):\n",
        "        cur.execute(\n",
        "            \"UPDATE DimCompany SET company_name = ? WHERE company_id = ?\",\n",
        "            row.company_name, row.company_id\n",
        "        )\n",
        "    conn.commit()\n",
        "print(f\"[DEBUG] DimCompany updated with {len(merged)} company_name entries.\")\n",
        "\n",
        "# 4. Insert new dates into DimDate\n",
        "news_dates = news_df['published_at'].dt.normalize().drop_duplicates().to_frame(name='date')\n",
        "news_dates['date_id'] = news_dates['date'].dt.strftime('%Y%m%d').astype(int)\n",
        "news_dates['day'] = news_dates['date'].dt.day\n",
        "news_dates['month'] = news_dates['date'].dt.month\n",
        "news_dates['month_name'] = news_dates['date'].dt.strftime('%B')\n",
        "news_dates['quarter'] = news_dates['date'].dt.quarter\n",
        "news_dates['year'] = news_dates['date'].dt.year\n",
        "news_dates['weekday'] = news_dates['date'].dt.day_name()\n",
        "\n",
        "with conn.cursor() as cur:\n",
        "    cur.execute('SELECT date_id FROM DimDate')\n",
        "    existing_date_ids = set(row[0] for row in cur.fetchall())\n",
        "    new_dates = news_dates[~news_dates['date_id'].isin(existing_date_ids)]\n",
        "    if not new_dates.empty:\n",
        "        batch_insert(\n",
        "            cur,\n",
        "            \"INSERT INTO DimDate (date_id, day, month, month_name, quarter, year, weekday) VALUES (?, ?, ?, ?, ?, ?, ?)\",\n",
        "            new_dates[['date_id', 'day', 'month', 'month_name', 'quarter', 'year', 'weekday']].values.tolist()\n",
        "        )\n",
        "        conn.commit()\n",
        "\n",
        "# 5. Create FactNews table if not exists (without article column in fact table)\n",
        "with conn.cursor() as cur:\n",
        "    cur.execute(\"\"\"\n",
        "        IF NOT EXISTS (\n",
        "            SELECT * FROM INFORMATION_SCHEMA.TABLES\n",
        "            WHERE TABLE_NAME = 'FactNews'\n",
        "        )\n",
        "        BEGIN\n",
        "            CREATE TABLE FactNews (\n",
        "                fact_id INT IDENTITY(1,1) PRIMARY KEY,\n",
        "                date_id INT,\n",
        "                company_id INT,\n",
        "                sentiment_id INT,\n",
        "                source_url VARCHAR(MAX),\n",
        "                FOREIGN KEY (date_id) REFERENCES DimDate(date_id),\n",
        "                FOREIGN KEY (company_id) REFERENCES DimCompany(company_id),\n",
        "                FOREIGN KEY (sentiment_id) REFERENCES DimSentiment(sentiment_id)\n",
        "            )\n",
        "        END\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "\n",
        "# 6. Prepare FactNews data (drop article column)\n",
        "with conn.cursor() as cur:\n",
        "    cur.execute('SELECT ticker, company_id FROM DimCompany')\n",
        "    company_map = {row[0]: row[1] for row in cur.fetchall()}\n",
        "    cur.execute('SELECT sentiment, sentiment_id FROM DimSentiment')\n",
        "    sentiment_map = {row[0].lower(): row[1] for row in cur.fetchall()}\n",
        "\n",
        "news_df['company_id'] = news_df['ticker_symbol'].map(company_map)\n",
        "news_df['sentiment_id'] = news_df['sentiment'].str.lower().map(sentiment_map)\n",
        "\n",
        "fact_news = news_df[['date_id', 'company_id', 'sentiment_id', 'source_url']].copy()\n",
        "fact_news = fact_news.dropna(subset=['date_id', 'company_id', 'sentiment_id'])\n",
        "\n",
        "# Convert IDs explicitly to int\n",
        "fact_news['date_id'] = fact_news['date_id'].astype(int)\n",
        "fact_news['company_id'] = fact_news['company_id'].astype(int)\n",
        "fact_news['sentiment_id'] = fact_news['sentiment_id'].astype(int)\n",
        "\n",
        "print(\"[DEBUG] fact_news null counts:\")\n",
        "print(fact_news.isnull().sum())\n",
        "print(\"[DEBUG] fact_news dtypes:\")\n",
        "print(fact_news.dtypes)\n",
        "print(\"[DEBUG] Inserting\", len(fact_news), \"rows into FactNews...\")\n",
        "print(\"[DEBUG] Sample row:\", fact_news.iloc[0].to_dict())\n",
        "\n",
        "# 7. Insert into FactNews\n",
        "with conn.cursor() as cur:\n",
        "    if not fact_news.empty:\n",
        "        batch_insert(\n",
        "            cur,\n",
        "            \"INSERT INTO FactNews (date_id, company_id, sentiment_id, source_url) VALUES (?, ?, ?, ?)\",\n",
        "            fact_news.values.tolist()\n",
        "        )\n",
        "        conn.commit()\n",
        "    else:\n",
        "        print('[DEBUG] No valid rows to insert into FactNews.')\n",
        "\n",
        "conn.close()\n",
        "end = time.time()\n",
        "print(f\"All tables created/updated in {end - start:.2f} seconds.\")"
      ],
      "metadata": {
        "id": "b0jiJDICAx8f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}